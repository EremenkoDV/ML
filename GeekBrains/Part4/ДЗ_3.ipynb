{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ДЗ_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNKzG9Ze5KKh"
      },
      "source": [
        "# Урок 3. Построение модели классификации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiuAumsk5NMu"
      },
      "source": [
        "1. **Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhqKsOQr5Z-U"
      },
      "source": [
        "\n",
        "*   micro-среднее используется для вычисления метрик f1/AUC/precision/recall, учитывает общее количество истинных положительных, ложных отрицательных и ложных положительных результатов (независимо от прогноза для каждого класса в наборе данных).\n",
        "*   macro-среднее используется для вычисления метрик f1/AUC/precision/recall для каждого класса, и возвращает среднее значение без учета пропорции для каждого класса в наборе данных.\n",
        "*   weighted-среднее используется для вычисления метрик f1/AUC/precision/recall для каждого класса, и возвращает среднее значение с учетом пропорции для каждого класса в наборе данных.\n",
        "\n",
        "Макро-среднее вычисляет метрику независимо для каждого класса, а затем берет среднее, т.е. будет обрабатывать все классы одинаково, тогда как микро-среднее будет агрегировать вклады всех классов для вычисления средней метрики. В мультиклассовой классификации предпочтение отдается микро-среднему, если есть подозрение, что имеется дисбаланс классов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "263-b4Mrni-R"
      },
      "source": [
        "2. **В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CRohXsHni-R"
      },
      "source": [
        "Особенности моделей XGBoost, LightGBM, CatBoost:\n",
        "\n",
        "*   XGBoost (eXtreme Gradient Boosting) имеет отличия на уровне построения каждого дерева от обычного алгоритма Gradient Boosting:\n",
        "    *   Расчет ведется на определении схожести объектов, попавших на очередной уровень дерева, в который уже заложена регуляризация (лямбда), позволяющая бороться с переобучением. В отличие от обычного бустинга, где прирост информативности основывается на расчете или критерия Шеннона (энтропии), или на критерии Джини.\n",
        "    *   Дополнительно параметр гамма позволяет делать \"стрижку\" переобучаемых деревьев.\n",
        "\n",
        "*   LightGBM от разработчиков Microsoft нацелен прежде всего на скорость обучения благодаря своим 3 основных технологиям, отличающих его от других моделей Boost-семейства:\n",
        "    *   Gradient-based One-Side Sampling (GOSS) - данная технология на каждом этапе формирует выборку для следующего, сортируя объекты с наибольшим градиентом, выбирая часть из данной выборки, добавляя выборку из объектов с низким градиентом. Таким образом, следующий этап выполняется не по всем объектам, а лишь наибольшей ошибкой с добавлением объектов с низкой ошибкой.\n",
        "    *   Exclusive Feature Bundling (EFB) - еще одна технология, позволяющая повысить скорость обучения модели путем объединения нескольких столбцов (признаков) в один, сдвигая значения на неперекрывающиеся диапазоны.\n",
        "    *   Построение ветви дерева от листа с наибольшим значением ошибки, а не общим уровнем, как у обычного дерева решений, позволяет расчету быстрее свести наибольшую ошибку к минимуму.\n",
        "\n",
        "*   CatBoost от разработчиков Яндекс строит деревья симмитричными - на каждом уровне задаются одинаковые вопросы, что позволяет производить наиболее точный и быстрый поиск. Кроме того, алгоритм позволяет предварительно не обрабатывать категориальные признаки, алгоритм обнаружит и подготовит из сам. Также алгоритм сам старается подобрать наиболее эффективные гиперпараметры. Предотвращение переобучения достигается за счет обучения нескольких моделей за одну итерацию, при этом постоянно выполняются перемешивания выборок, обеспечивания дополнительную независимость обучения. Данный алгоритм зачастую превосходит LightGBM по точности предсказания, но проигрывает ему по скорости.\n",
        "\n",
        "\n"
      ]
    }
  ]
}